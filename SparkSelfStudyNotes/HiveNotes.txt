TMP and TEMP
%USERPROFILE%\AppData\Local\Temp



Create hive table:
drop table hive_table;
CREATE TABLE hive_table(  empno INT,  ename string,  designation string,  sal Double,  manager INT,  deptno INT)ROW FORMAT DELIMITED  FIELDS TERMINATED BY '\t'  LINES TERMINATED BY '\n' STORED AS TEXTFILE;


load data to hive table:

LOAD DATA LOCAL INPATH '/home/training/dvs/employee_hive_hbase.csv' INTO TABLE hive_table;

create HBase-hive Mapping Table

CREATE TABLE hbase_table_employee(empno INT, ename STRING, designation STRING, sal Double, manager INT, deptno INT ) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key, cf:ename, cf:designation, cf:sal,  cf:manager, cf:deptno") TBLPROPERTIES ("hbase.table.name" = "employee_hbase");

INSERT INTO TABLE hbase_table_employee SELECT * FROM hive_table;

LOAD DATA LOCAL INPATH '/home/training/dvs/employee_hive_hbase.csv' INTO TABLE hbase_table_employee;
======= SET 
[training@localhost ~]$ hive
WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
Logging initialized using configuration in jar:file:/home/training/hive/lib/hive-common-0.10.0.jar!/hive-log4j.properties
Hive history file=/tmp/training/hive_job_log_training_201912280111_1662152489.txt
hive (default)> set;
datanucleus.autoCreateSchema=true
datanucleus.autoStartMechanismMode=checked
datanucleus.cache.level2=false
datanucleus.cache.level2.type=SOFT
datanucleus.connectionPoolingType=DBCP
datanucleus.identifierFactory=datanucleus
datanucleus.plugin.pluginRegistryBundleCheck=LOG
datanucleus.storeManagerType=rdbms
datanucleus.transactionIsolation=read-committed
datanucleus.validateColumns=false
datanucleus.validateConstraints=false
datanucleus.validateTables=false
fs.har.impl=org.apache.hadoop.hive.shims.HiveHarFileSystem
hadoop.bin.path=/usr/local/hadoop/libexec/../bin/hadoop
hive.archive.enabled=false
hive.auto.convert.join=false
hive.auto.progress.timeout=0
hive.autogen.columnalias.prefix.includefuncname=false
hive.autogen.columnalias.prefix.label=_c
hive.binary.record.max.length=1000
hive.cli.errors.ignore=false
hive.cli.print.current.db=true
hive.cli.print.header=true
hive.cli.prompt=hive
hive.cluster.delegation.token.store.class=org.apache.hadoop.hive.thrift.MemoryTokenStore
hive.cluster.delegation.token.store.zookeeper.acl=sasl:hive/host1@EXAMPLE.COM:cdrwa,sasl:hive/host2@EXAMPLE.COM:cdrwa
hive.cluster.delegation.token.store.zookeeper.connectString=localhost:2181
hive.cluster.delegation.token.store.zookeeper.znode=/hive/cluster/delegation
hive.conf.validation=true
hive.ddl.output.format=text
hive.debug.localtask=false
hive.default.fileformat=TextFile
hive.downloaded.resources.dir=/tmp/training/hive_resources
hive.enforce.bucketing=false
hive.enforce.bucketmapjoin=false
hive.enforce.sorting=false
hive.enforce.sortmergebucketmapjoin=false
hive.entity.separator=@
hive.error.on.empty.partition=false
hive.exec.compress.intermediate=false
hive.exec.compress.output=false
hive.exec.concatenate.check.index=true
hive.exec.counters.pull.interval=1000
hive.exec.default.partition.name=__HIVE_DEFAULT_PARTITION__
hive.exec.drop.ignorenonexistent=true
hive.exec.dynamic.partition=true
hive.exec.dynamic.partition.mode=strict
hive.exec.job.debug.capture.stacktraces=true
hive.exec.job.debug.timeout=30000
hive.exec.list.bucketing.default.dir=HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME
hive.exec.local.scratchdir=/tmp/${user.name}
hive.exec.max.created.files=100000
hive.exec.max.dynamic.partitions=1000
hive.exec.max.dynamic.partitions.pernode=100
hive.exec.mode.local.auto=false
hive.exec.mode.local.auto.input.files.max=4
hive.exec.mode.local.auto.inputbytes.max=134217728
hive.exec.parallel=false
hive.exec.parallel.thread.number=8
hive.exec.perf.logger=org.apache.hadoop.hive.ql.log.PerfLogger
hive.exec.rcfile.use.explicit.header=true
hive.exec.reducers.bytes.per.reducer=1000000000
hive.exec.reducers.max=999
hive.exec.rowoffset=false
hive.exec.scratchdir=/tmp/hive-${user.name}
hive.exec.script.allow.partial.consumption=false
hive.exec.script.maxerrsize=100000
hive.exec.script.trust=false
hive.exec.show.job.failure.debug.info=true
hive.exec.submitviachild=false
hive.exec.tasklog.debug.timeout=20000
hive.exim.uri.scheme.whitelist=hdfs,pfile
hive.fetch.output.serde=org.apache.hadoop.hive.serde2.DelimitedJSONSerDe
hive.fetch.task.conversion=minimal
hive.fileformat.check=true
hive.groupby.mapaggr.checkinterval=100000
hive.groupby.skewindata=false
hive.hadoop.supports.splittable.combineinputformat=false
hive.hashtable.initialCapacity=100000
hive.hashtable.loadfactor=0.75
hive.hbase.wal.enabled=true
hive.heartbeat.interval=1000
hive.hmshandler.force.reload.conf=false
hive.hmshandler.retry.attempts=1
hive.hmshandler.retry.interval=1000
hive.hwi.listen.host=0.0.0.0
hive.hwi.listen.port=9999
hive.hwi.war.file=lib/hive-hwi-0.10.0.war
hive.index.compact.binary.search=true
hive.index.compact.file.ignore.hdfs=false
hive.index.compact.query.max.entries=10000000
hive.index.compact.query.max.size=10737418240
hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat
hive.insert.into.external.tables=true
hive.insert.into.multilevel.dirs=false
hive.internal.ddl.list.bucketing.enable=false
hive.jobname.length=50
hive.join.cache.size=25000
hive.join.emit.interval=1000
hive.limit.optimize.enable=false
hive.limit.optimize.fetch.max=50000
hive.limit.optimize.limit.file=10
hive.limit.row.max.size=100000
hive.lock.manager=org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager
hive.lock.mapred.only.operation=false
hive.lock.numretries=100
hive.lock.sleep.between.retries=60
hive.lockmgr.zookeeper.default.partition.name=__HIVE_DEFAULT_ZOOKEEPER_PARTITION__
hive.map.aggr=true
hive.map.aggr.hash.force.flush.memory.threshold=0.9
hive.map.aggr.hash.min.reduction=0.5
hive.map.aggr.hash.percentmemory=0.5
hive.map.groupby.sorted=false
hive.mapjoin.bucket.cache.size=100
hive.mapjoin.cache.numrows=25000
hive.mapjoin.check.memory.rows=100000
hive.mapjoin.followby.gby.localtask.max.memory.usage=0.55
hive.mapjoin.followby.map.aggr.hash.percentmemory=0.3
hive.mapjoin.localtask.max.memory.usage=0.90
hive.mapjoin.size.key=10000
hive.mapjoin.smalltable.filesize=25000000
hive.mapper.cannot.span.multiple.partitions=false
hive.mapred.local.mem=0
hive.mapred.mode=nonstrict
hive.mapred.partitioner=org.apache.hadoop.hive.ql.io.DefaultHivePartitioner
hive.mapred.reduce.tasks.speculative.execution=true
hive.mapred.supports.subdirectories=false
hive.merge.current.job.has.dynamic.partitions=false
hive.merge.input.format.block.level=org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileBlockMergeInputFormat
hive.merge.mapfiles=true
hive.merge.mapredfiles=false
hive.merge.rcfile.block.level=true
hive.merge.size.per.task=256000000
hive.merge.smallfiles.avgsize=16000000
hive.mergejob.maponly=true
hive.metadata.move.exported.metadata.to.trash=true
hive.metastore.archive.intermediate.archived=_INTERMEDIATE_ARCHIVED
hive.metastore.archive.intermediate.extracted=_INTERMEDIATE_EXTRACTED
hive.metastore.archive.intermediate.original=_INTERMEDIATE_ORIGINAL
hive.metastore.authorization.storage.checks=false
hive.metastore.batch.retrieve.max=300
hive.metastore.batch.retrieve.table.partition.max=1000
hive.metastore.cache.pinobjtypes=Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order
hive.metastore.client.connect.retry.delay=1
hive.metastore.client.socket.timeout=20
hive.metastore.connect.retries=5
hive.metastore.ds.retry.attempts=1
hive.metastore.ds.retry.interval=1000
hive.metastore.event.clean.freq=0
hive.metastore.event.expiry.duration=0
hive.metastore.execute.setugi=false
hive.metastore.failure.retries=3
hive.metastore.force.reload.conf=false
hive.metastore.fs.handler.class=org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl
hive.metastore.kerberos.principal=hive-metastore/_HOST@EXAMPLE.COM
hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore
hive.metastore.sasl.enabled=false
hive.metastore.server.max.threads=100000
hive.metastore.server.min.threads=200
hive.metastore.server.tcp.keepalive=true
hive.metastore.thrift.framed.transport.enabled=false
hive.metastore.warehouse.dir=/user/hive/warehouse
hive.multi.insert.move.tasks.share.dependencies=false
hive.multigroupby.singlemr=false
hive.multigroupby.singlereducer=true
hive.optimize.bucketmapjoin=false
hive.optimize.bucketmapjoin.sortedmerge=false
hive.optimize.cp=true
hive.optimize.groupby=true
hive.optimize.index.autoupdate=false
hive.optimize.index.filter=false
hive.optimize.index.filter.compact.maxsize=-1
hive.optimize.index.filter.compact.minsize=5368709120
hive.optimize.index.groupby=false
hive.optimize.listbucketing=false
hive.optimize.metadataonly=true
hive.optimize.ppd=true
hive.optimize.ppd.storage=true
hive.optimize.reducededuplication=true
hive.optimize.skewjoin=false
hive.optimize.skewjoin.compiletime=false
hive.optimize.union.remove=false
hive.outerjoin.supports.filters=true
hive.ppd.recognizetransivity=true
hive.ppd.remove.duplicatefilters=true
hive.query.result.fileformat=TextFile
hive.querylog.enable.plan.progress=true
hive.querylog.location=/tmp/${user.name}
hive.querylog.plan.progress.interval=60000
hive.rework.mapredwork=false
hive.sample.seednumber=0
hive.script.auto.progress=false
hive.script.operator.id.env.var=HIVE_SCRIPT_OPERATOR_ID
hive.script.operator.truncate.env=false
hive.script.recordreader=org.apache.hadoop.hive.ql.exec.TextRecordReader
hive.script.recordwriter=org.apache.hadoop.hive.ql.exec.TextRecordWriter
hive.script.serde=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator
hive.security.authorization.enabled=false
hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider
hive.security.metastore.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator
hive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider
hive.server.read.socket.timeout=10
hive.server.tcp.keepalive=true
hive.session.id=training_201912280111
hive.session.silent=false
hive.skewjoin.key=100000
hive.skewjoin.mapjoin.map.tasks=10000
hive.skewjoin.mapjoin.min.split=33554432
hive.start.cleanup.scratchdir=false
hive.stats.atomic=false
hive.stats.autogather=true
hive.stats.collect.rawdatasize=true
hive.stats.collect.tablekeys=false
hive.stats.dbclass=jdbc:derby
hive.stats.dbconnectionstring=jdbc:derby:;databaseName=TempStatsStore;create=true
hive.stats.jdbc.timeout=30
hive.stats.jdbcdriver=org.apache.derby.jdbc.EmbeddedDriver
hive.stats.ndv.error=20.0
hive.stats.reliable=false
hive.stats.retries.max=0
hive.stats.retries.wait=3000
hive.support.concurrency=false
hive.task.progress=false
hive.test.mode=false
hive.test.mode.prefix=test_
hive.test.mode.samplefreq=32
hive.transform.escape.input=false
hive.udtf.auto.progress=false
hive.unlock.numretries=10
hive.variable.substitute=true
hive.variable.substitute.depth=40
hive.warehouse.subdir.inherit.perms=false
hive.zookeeper.clean.extra.nodes=false
hive.zookeeper.client.port=2181
hive.zookeeper.namespace=hive_zookeeper_namespace
hive.zookeeper.session.timeout=600000
javax.jdo.PersistenceManagerFactoryClass=org.datanucleus.jdo.JDOPersistenceManagerFactory
javax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver
javax.jdo.option.ConnectionPassword=hadoop
javax.jdo.option.ConnectionURL=jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true
javax.jdo.option.ConnectionUserName=hadoop
javax.jdo.option.DetachAllOnCommit=true
javax.jdo.option.Multithreaded=true
javax.jdo.option.NonTransactionalRead=true
mapred.input.dir.recursive=false
mapred.max.split.size=256000000
mapred.min.split.size=1
mapred.min.split.size.per.node=1
mapred.min.split.size.per.rack=1
mapred.reduce.tasks=-1
silent=off
env:COLORTERM=gnome-terminal
env:DBUS_SESSION_BUS_ADDRESS=unix:abstract=/tmp/dbus-qTpPupBQsJ,guid=e599078276edd0bb9164b13a00000063
env:DESKTOP_SESSION=gnome
env:DISPLAY=:0.0
env:GDMSESSION=gnome
env:GDM_KEYBOARD_LAYOUT=us
env:GDM_LANG=en_US.utf8
env:GNOME_DESKTOP_SESSION_ID=this-is-deprecated
env:GNOME_KEYRING_PID=2662
env:GNOME_KEYRING_SOCKET=/tmp/keyring-z5poGR/socket
env:GTK_RC_FILES=/etc/gtk/gtkrc:/home/training/.gtkrc-1.2-gnome2
env:G_BROKEN_FILENAMES=1
env:HADOOP_BALANCER_OPTS=-Dcom.sun.management.jmxremote 
env:HADOOP_CLASSPATH=:/home/training/hive/conf:/home/training/hive/lib/antlr-2.7.7.jar:/home/training/hive/lib/antlr-runtime-3.0.1.jar:/home/training/hive/lib/avro-1.7.1.jar:/home/training/hive/lib/avro-mapred-1.7.1.jar:/home/training/hive/lib/commons-cli-1.2.jar:/home/training/hive/lib/commons-codec-1.4.jar:/home/training/hive/lib/commons-collections-3.2.1.jar:/home/training/hive/lib/commons-compress-1.4.1.jar:/home/training/hive/lib/commons-configuration-1.6.jar:/home/training/hive/lib/commons-dbcp-1.4.jar:/home/training/hive/lib/commons-lang-2.4.jar:/home/training/hive/lib/commons-logging-1.0.4.jar:/home/training/hive/lib/commons-logging-api-1.0.4.jar:/home/training/hive/lib/commons-pool-1.5.4.jar:/home/training/hive/lib/datanucleus-connectionpool-2.0.3.jar:/home/training/hive/lib/datanucleus-core-2.0.3.jar:/home/training/hive/lib/datanucleus-enhancer-2.0.3.jar:/home/training/hive/lib/datanucleus-rdbms-2.0.3.jar:/home/training/hive/lib/derby-10.4.2.0.jar:/home/training/hive/lib/guava-r09.jar:/home/training/hive/lib/hbase-0.92.0.jar:/home/training/hive/lib/hbase-0.92.0-tests.jar:/home/training/hive/lib/hive-builtins-0.10.0.jar:/home/training/hive/lib/hive-cli-0.10.0.jar:/home/training/hive/lib/hive-common-0.10.0.jar:/home/training/hive/lib/hive-contrib-0.10.0.jar:/home/training/hive/lib/hive-exec-0.10.0.jar:/home/training/hive/lib/hive-hbase-handler-0.10.0.jar:/home/training/hive/lib/hive-hwi-0.10.0.jar:/home/training/hive/lib/hive-jdbc-0.10.0.jar:/home/training/hive/lib/hive-metastore-0.10.0.jar:/home/training/hive/lib/hive-pdk-0.10.0.jar:/home/training/hive/lib/hive-serde-0.10.0.jar:/home/training/hive/lib/hive-service-0.10.0.jar:/home/training/hive/lib/hive-shims-0.10.0.jar:/home/training/hive/lib/jackson-core-asl-1.8.8.jar:/home/training/hive/lib/jackson-jaxrs-1.8.8.jar:/home/training/hive/lib/jackson-mapper-asl-1.8.8.jar:/home/training/hive/lib/jackson-xc-1.8.8.jar:/home/training/hive/lib/JavaEWAH-0.3.2.jar:/home/training/hive/lib/javolution-5.5.1.jar:/home/training/hive/lib/jdo2-api-2.3-ec.jar:/home/training/hive/lib/jetty-6.1.26.jar:/home/training/hive/lib/jetty-util-6.1.26.jar:/home/training/hive/lib/jline-0.9.94.jar:/home/training/hive/lib/json-20090211.jar:/home/training/hive/lib/libfb303-0.9.0.jar:/home/training/hive/lib/libthrift-0.9.0.jar:/home/training/hive/lib/log4j-1.2.16.jar:/home/training/hive/lib/mysql-connector-java-5.1.18.jar:/home/training/hive/lib/ojdbc14.jar:/home/training/hive/lib/servlet-api-2.5-20081211.jar:/home/training/hive/lib/slf4j-api-1.6.1.jar:/home/training/hive/lib/slf4j-log4j12-1.6.1.jar:/home/training/hive/lib/sqlline-1_0_2.jar:/home/training/hive/lib/stringtemplate-3.1-b1.jar:/home/training/hive/lib/xz-1.0.jar:/home/training/hive/lib/zookeeper-3.4.3.jar:
env:HADOOP_DATANODE_OPTS=-Dcom.sun.management.jmxremote 
env:HADOOP_HEAPSIZE=256
env:HADOOP_HOME=/usr/local/hadoop/libexec/..
env:HADOOP_HOME_WARN_SUPPRESS=1
env:HADOOP_JOBTRACKER_OPTS=-Dcom.sun.management.jmxremote 
env:HADOOP_LIB=/usr/local/hadoop/lib
env:HADOOP_NAMENODE_OPTS=-Dcom.sun.management.jmxremote 
env:HADOOP_PREFIX=/usr/local/hadoop/libexec/..
env:HADOOP_SECONDARYNAMENODE_OPTS=-Dcom.sun.management.jmxremote 
env:HBASE_HOME=/home/training/hbase
env:HBASE_LOG_DIR=/home/training/hbase/logs
env:HISTCONTROL=ignoredups
env:HISTSIZE=1000
env:HIVE_AUX_JARS_PATH=
env:HIVE_CONF_DIR=/home/training/hive/conf
env:HIVE_HOME=/home/training/hive
env:HOME=/home/training
env:HOSTNAME=localhost.localdomain
env:JAVA_HOME=/usr/java/jdk1.7.0_10
env:LANG=en_US.utf8
env:LESSOPEN=|/usr/bin/lesspipe.sh %s
env:LOGNAME=training
env:LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lz=01;31:*.xz=01;31:*.bz2=01;31:*.tbz=01;31:*.tbz2=01;31:*.bz=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.rar=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:
env:MAIL=/var/spool/mail/training
env:NLSPATH=/usr/dt/lib/nls/msg/%L/%N.cat
env:ORBIT_SOCKETDIR=/tmp/orbit-training
env:PATH=/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/sbin:/usr/java/jdk1.7.0_10/bin:/usr/local/hadoop/bin:/home/training/hive/bin:/home/training/pig-0.10.0/bin::/home/training/Sqoop/bin:/home/training/bin:/usr/java/jdk1.7.0_10/bin:/usr/local/hadoop/bin:/home/training/hive/bin:/home/training/pig-0.10.0/bin::/home/training/Sqoop/bin:/home/training/hbase/bin:/usr/java/jdk1.7.0_10/bin:/usr/local/hadoop/bin:/home/training/hive/bin:/home/training/pig-0.10.0/bin::/home/training/Sqoop/bin:/home/training/bin:/usr/java/jdk1.7.0_10/bin:/usr/local/hadoop/bin:/home/training/hive/bin:/home/training/pig-0.10.0/bin::/home/training/Sqoop/bin
env:PIG_HOME=/home/training/pig-0.10.0
env:PWD=/home/training
env:SERVICE_LIST=beeline cli help hiveserver hwi jar lineage metastore metatool rcfilecat 
env:SESSION_MANAGER=local/unix:@/tmp/.ICE-unix/2677,unix/unix:/tmp/.ICE-unix/2677
env:SHELL=/bin/bash
env:SHLVL=2
env:SQOOP_HOME=/home/training/Sqoop
env:SSH_ASKPASS=/usr/libexec/openssh/gnome-ssh-askpass
env:SSH_AUTH_SOCK=/tmp/keyring-z5poGR/socket.ssh
env:TERM=xterm
env:USER=training
env:USERNAME=training
env:WINDOWID=48310033
env:WINDOWPATH=1
env:XAUTHORITY=/var/run/gdm/auth-for-training-troFG9/database
env:XDG_SESSION_COOKIE=dde049a30dd017107bde4f8300000014-1577335566.37144-1480371484
env:XFILESEARCHPATH=/usr/dt/app-defaults/%L/Dt
system:awt.toolkit=sun.awt.X11.XToolkit
system:file.encoding=UTF-8
system:file.encoding.pkg=sun.io
system:file.separator=/
system:hadoop.home.dir=/usr/local/hadoop/libexec/..
system:hadoop.id.str=
system:hadoop.log.dir=/usr/local/hadoop/libexec/../logs
system:hadoop.log.file=hadoop.log
system:hadoop.policy.file=hadoop-policy.xml
system:hadoop.root.logger=INFO,console
system:hadoop.security.logger=INFO,NullAppender
system:java.awt.graphicsenv=sun.awt.X11GraphicsEnvironment
system:java.awt.printerjob=sun.print.PSPrinterJob
system:java.class.path=/usr/local/hadoop/libexec/../conf:/usr/java/jdk1.7.0_10/lib/tools.jar:/usr/local/hadoop/libexec/..:/usr/local/hadoop/libexec/../hadoop-core-1.0.3.jar:/usr/local/hadoop/libexec/../lib/asm-3.2.jar:/usr/local/hadoop/libexec/../lib/aspectjrt-1.6.5.jar:/usr/local/hadoop/libexec/../lib/aspectjtools-1.6.5.jar:/usr/local/hadoop/libexec/../lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/libexec/../lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/libexec/../lib/commons-cli-1.2.jar:/usr/local/hadoop/libexec/../lib/commons-codec-1.4.jar:/usr/local/hadoop/libexec/../lib/commons-collections-3.2.1.jar:/usr/local/hadoop/libexec/../lib/commons-configuration-1.6.jar:/usr/local/hadoop/libexec/../lib/commons-daemon-1.0.1.jar:/usr/local/hadoop/libexec/../lib/commons-digester-1.8.jar:/usr/local/hadoop/libexec/../lib/commons-el-1.0.jar:/usr/local/hadoop/libexec/../lib/commons-httpclient-3.0.1.jar:/usr/local/hadoop/libexec/../lib/commons-io-2.1.jar:/usr/local/hadoop/libexec/../lib/commons-lang-2.4.jar:/usr/local/hadoop/libexec/../lib/commons-logging-1.1.1.jar:/usr/local/hadoop/libexec/../lib/commons-logging-api-1.0.4.jar:/usr/local/hadoop/libexec/../lib/commons-math-2.1.jar:/usr/local/hadoop/libexec/../lib/commons-net-1.4.1.jar:/usr/local/hadoop/libexec/../lib/core-3.1.1.jar:/usr/local/hadoop/libexec/../lib/hadoop-capacity-scheduler-1.0.3.jar:/usr/local/hadoop/libexec/../lib/hadoop-fairscheduler-1.0.3.jar:/usr/local/hadoop/libexec/../lib/hadoop-thriftfs-1.0.3.jar:/usr/local/hadoop/libexec/../lib/hsqldb-1.8.0.10.jar:/usr/local/hadoop/libexec/../lib/jackson-core-asl-1.8.8.jar:/usr/local/hadoop/libexec/../lib/jackson-mapper-asl-1.8.8.jar:/usr/local/hadoop/libexec/../lib/jasper-compiler-5.5.12.jar:/usr/local/hadoop/libexec/../lib/jasper-runtime-5.5.12.jar:/usr/local/hadoop/libexec/../lib/jdeb-0.8.jar:/usr/local/hadoop/libexec/../lib/jersey-core-1.8.jar:/usr/local/hadoop/libexec/../lib/jersey-json-1.8.jar:/usr/local/hadoop/libexec/../lib/jersey-server-1.8.jar:/usr/local/hadoop/libexec/../lib/jets3t-0.6.1.jar:/usr/local/hadoop/libexec/../lib/jetty-6.1.26.jar:/usr/local/hadoop/libexec/../lib/jetty-util-6.1.26.jar:/usr/local/hadoop/libexec/../lib/jsch-0.1.42.jar:/usr/local/hadoop/libexec/../lib/junit-4.5.jar:/usr/local/hadoop/libexec/../lib/kfs-0.2.2.jar:/usr/local/hadoop/libexec/../lib/log4j-1.2.15.jar:/usr/local/hadoop/libexec/../lib/mockito-all-1.8.5.jar:/usr/local/hadoop/libexec/../lib/oro-2.0.8.jar:/usr/local/hadoop/libexec/../lib/servlet-api-2.5-20081211.jar:/usr/local/hadoop/libexec/../lib/slf4j-api-1.4.3.jar:/usr/local/hadoop/libexec/../lib/slf4j-log4j12-1.4.3.jar:/usr/local/hadoop/libexec/../lib/xmlenc-0.52.jar:/usr/local/hadoop/libexec/../lib/jsp-2.1/jsp-2.1.jar:/usr/local/hadoop/libexec/../lib/jsp-2.1/jsp-api-2.1.jar::/home/training/hive/conf:/home/training/hive/lib/antlr-2.7.7.jar:/home/training/hive/lib/antlr-runtime-3.0.1.jar:/home/training/hive/lib/avro-1.7.1.jar:/home/training/hive/lib/avro-mapred-1.7.1.jar:/home/training/hive/lib/commons-cli-1.2.jar:/home/training/hive/lib/commons-codec-1.4.jar:/home/training/hive/lib/commons-collections-3.2.1.jar:/home/training/hive/lib/commons-compress-1.4.1.jar:/home/training/hive/lib/commons-configuration-1.6.jar:/home/training/hive/lib/commons-dbcp-1.4.jar:/home/training/hive/lib/commons-lang-2.4.jar:/home/training/hive/lib/commons-logging-1.0.4.jar:/home/training/hive/lib/commons-logging-api-1.0.4.jar:/home/training/hive/lib/commons-pool-1.5.4.jar:/home/training/hive/lib/datanucleus-connectionpool-2.0.3.jar:/home/training/hive/lib/datanucleus-core-2.0.3.jar:/home/training/hive/lib/datanucleus-enhancer-2.0.3.jar:/home/training/hive/lib/datanucleus-rdbms-2.0.3.jar:/home/training/hive/lib/derby-10.4.2.0.jar:/home/training/hive/lib/guava-r09.jar:/home/training/hive/lib/hbase-0.92.0.jar:/home/training/hive/lib/hbase-0.92.0-tests.jar:/home/training/hive/lib/hive-builtins-0.10.0.jar:/home/training/hive/lib/hive-cli-0.10.0.jar:/home/training/hive/lib/hive-common-0.10.0.jar:/home/training/hive/lib/hive-contrib-0.10.0.jar:/home/training/hive/lib/hive-exec-0.10.0.jar:/home/training/hive/lib/hive-hbase-handler-0.10.0.jar:/home/training/hive/lib/hive-hwi-0.10.0.jar:/home/training/hive/lib/hive-jdbc-0.10.0.jar:/home/training/hive/lib/hive-metastore-0.10.0.jar:/home/training/hive/lib/hive-pdk-0.10.0.jar:/home/training/hive/lib/hive-serde-0.10.0.jar:/home/training/hive/lib/hive-service-0.10.0.jar:/home/training/hive/lib/hive-shims-0.10.0.jar:/home/training/hive/lib/jackson-core-asl-1.8.8.jar:/home/training/hive/lib/jackson-jaxrs-1.8.8.jar:/home/training/hive/lib/jackson-mapper-asl-1.8.8.jar:/home/training/hive/lib/jackson-xc-1.8.8.jar:/home/training/hive/lib/JavaEWAH-0.3.2.jar:/home/training/hive/lib/javolution-5.5.1.jar:/home/training/hive/lib/jdo2-api-2.3-ec.jar:/home/training/hive/lib/jetty-6.1.26.jar:/home/training/hive/lib/jetty-util-6.1.26.jar:/home/training/hive/lib/jline-0.9.94.jar:/home/training/hive/lib/json-20090211.jar:/home/training/hive/lib/libfb303-0.9.0.jar:/home/training/hive/lib/libthrift-0.9.0.jar:/home/training/hive/lib/log4j-1.2.16.jar:/home/training/hive/lib/mysql-connector-java-5.1.18.jar:/home/training/hive/lib/ojdbc14.jar:/home/training/hive/lib/servlet-api-2.5-20081211.jar:/home/training/hive/lib/slf4j-api-1.6.1.jar:/home/training/hive/lib/slf4j-log4j12-1.6.1.jar:/home/training/hive/lib/sqlline-1_0_2.jar:/home/training/hive/lib/stringtemplate-3.1-b1.jar:/home/training/hive/lib/xz-1.0.jar:/home/training/hive/lib/zookeeper-3.4.3.jar:
system:java.class.version=51.0
system:java.endorsed.dirs=/usr/java/jdk1.7.0_10/jre/lib/endorsed
system:java.ext.dirs=/usr/java/jdk1.7.0_10/jre/lib/ext:/usr/java/packages/lib/ext
system:java.home=/usr/java/jdk1.7.0_10/jre
system:java.io.tmpdir=/tmp
system:java.library.path=/usr/local/hadoop/libexec/../lib/native/Linux-i386-32
system:java.runtime.name=Java(TM) SE Runtime Environment
system:java.runtime.version=1.7.0_10-b18
system:java.specification.name=Java Platform API Specification
system:java.specification.vendor=Oracle Corporation
system:java.specification.version=1.7
system:java.vendor=Oracle Corporation
system:java.vendor.url=http://java.oracle.com/
system:java.vendor.url.bug=http://bugreport.sun.com/bugreport/
system:java.version=1.7.0_10
system:java.vm.info=mixed mode
system:java.vm.name=Java HotSpot(TM) Client VM
system:java.vm.specification.name=Java Virtual Machine Specification
system:java.vm.specification.vendor=Oracle Corporation
system:java.vm.specification.version=1.7
system:java.vm.vendor=Oracle Corporation
system:java.vm.version=23.6-b04
system:line.separator=

system:os.arch=i386
system:os.name=Linux
system:os.version=2.6.32-279.el6.i686
system:path.separator=:
system:proc_jar=
system:sun.arch.data.model=32
system:sun.boot.class.path=/usr/java/jdk1.7.0_10/jre/lib/resources.jar:/usr/java/jdk1.7.0_10/jre/lib/rt.jar:/usr/java/jdk1.7.0_10/jre/lib/sunrsasign.jar:/usr/java/jdk1.7.0_10/jre/lib/jsse.jar:/usr/java/jdk1.7.0_10/jre/lib/jce.jar:/usr/java/jdk1.7.0_10/jre/lib/charsets.jar:/usr/java/jdk1.7.0_10/jre/lib/jfr.jar:/usr/java/jdk1.7.0_10/jre/classes
system:sun.boot.library.path=/usr/java/jdk1.7.0_10/jre/lib/i386
system:sun.cpu.endian=little
system:sun.cpu.isalist=
system:sun.desktop=gnome
system:sun.io.unicode.encoding=UnicodeLittle
system:sun.java.command=org.apache.hadoop.util.RunJar /home/training/hive/lib/hive-cli-0.10.0.jar org.apache.hadoop.hive.cli.CliDriver
system:sun.java.launcher=SUN_STANDARD
system:sun.jnu.encoding=UTF-8
system:sun.management.compiler=HotSpot Client Compiler
system:sun.os.patch.level=unknown
system:user.country=US
system:user.dir=/home/training
system:user.home=/home/training
system:user.language=en
system:user.name=training
system:user.timezone=Asia/Calcutta
hive (default)> 
